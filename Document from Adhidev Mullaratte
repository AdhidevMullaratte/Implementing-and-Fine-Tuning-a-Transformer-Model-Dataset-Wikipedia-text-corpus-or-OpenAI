# -*- coding: utf-8 -*-
"""GENAITUTORIAL6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15vmEweBUKvW6QPjsiJkEaTfAYYEccUb9
"""

!pip install transformers datasets torch matplotlib

# Install required libraries
!pip install -q transformers datasets accelerate torch matplotlib

# Import libraries
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, get_linear_schedule_with_warmup
from datasets import load_dataset
from torch.utils.data import DataLoader
from accelerate import Accelerator
import matplotlib.pyplot as plt
import math
from tqdm import tqdm

# Initialize accelerator (automatically handles GPU/TPU/multi-GPU)
accelerator = Accelerator()

# Configuration
MODEL_NAME = "distilgpt2"  # Faster than GPT-2
DATASET_NAME = "wikitext"
DATASET_CONFIG = "wikitext-2-raw-v1"
BATCH_SIZE = 8  # Adjust based on GPU memory
GRAD_ACCUM_STEPS = 2  # Effective batch size = BATCH_SIZE * GRAD_ACCUM_STEPS
MAX_SEQ_LENGTH = 256  # Reduce from 512 to speed up training
NUM_EPOCHS = 1  # Start with 1 epoch for quick testing
MAX_TRAIN_SAMPLES = None  # Set to 1000 for quick testing

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)

# Load and preprocess dataset
dataset = load_dataset(DATASET_NAME, DATASET_CONFIG)

def tokenize_function(examples):
    # Filter empty texts and tokenize
    examples["text"] = [t.strip() for t in examples["text"] if t.strip() != ""]
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=MAX_SEQ_LENGTH,
        padding="max_length"  # Pad to fixed length for efficiency
    )

tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=["text"],
    num_proc=4  # Parallel processing
)

if MAX_TRAIN_SAMPLES:
    train_dataset = tokenized_dataset["train"].select(range(MAX_TRAIN_SAMPLES))
else:
    train_dataset = tokenized_dataset["train"]

eval_dataset = tokenized_dataset["validation"]

# DataLoader setup
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

train_loader = DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    collate_fn=data_collator,
    num_workers=2,  # Reduce workers to avoid warnings
    pin_memory=True  # Faster data transfer to GPU
)

eval_loader = DataLoader(
    eval_dataset,
    batch_size=BATCH_SIZE,
    collate_fn=data_collator,
    num_workers=2,
    pin_memory=True
)

# Optimizer and scheduler
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)
total_steps = len(train_loader) // GRAD_ACCUM_STEPS * NUM_EPOCHS
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=100,
    num_training_steps=total_steps
)

# Prepare with Accelerator (handles mixed precision and GPU distribution)
model, optimizer, train_loader, eval_loader = accelerator.prepare(
    model, optimizer, train_loader, eval_loader
)

# Training loop
train_losses = []
eval_losses = []

for epoch in range(NUM_EPOCHS):
    # Training
    model.train()
    total_loss = 0
    progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1} [Training]", leave=False)

    for step, batch in enumerate(progress_bar):
        with accelerator.accumulate(model):
            outputs = model(**batch)
            loss = outputs.loss
            accelerator.backward(loss)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()

            total_loss += loss.detach().float().item()
            progress_bar.set_postfix({"loss": f"{loss.item():.3f}"})

    avg_train_loss = total_loss / len(train_loader)
    train_losses.append(avg_train_loss)

    # Evaluation
    model.eval()
    total_eval_loss = 0
    for batch in tqdm(eval_loader, desc=f"Epoch {epoch+1} [Evaluation]", leave=False):
        with torch.no_grad():
            outputs = model(**batch)
            total_eval_loss += outputs.loss.item()

    avg_eval_loss = total_eval_loss / len(eval_loader)
    eval_losses.append(avg_eval_loss)
    perplexity = math.exp(avg_eval_loss)

    accelerator.print(
        f"Epoch {epoch+1} | "
        f"Train Loss: {avg_train_loss:.3f} | "
        f"Val Loss: {avg_eval_loss:.3f} | "
        f"Val Perplexity: {perplexity:.3f}"
    )

# Save the model
accelerator.wait_for_everyone()
accelerator.save_model(model, "fine_tuned_distilgpt2")

# Plot training curves
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label="Training Loss")
plt.plot(eval_losses, label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()

# Text generation demo
def generate_text(prompt, model, max_length=50):
    inputs = tokenizer(prompt, return_tensors="pt").to(accelerator.device)
    outputs = model.generate(
        inputs.input_ids,
        max_length=max_length,
        num_return_sequences=1,
        do_sample=True,
        top_k=50,
        attention_mask=inputs.attention_mask,  # Add attention mask
        pad_token_id=tokenizer.eos_token_id  # Explicitly set pad token
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

print("\nBefore fine-tuning:")
original_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(accelerator.device)
print(generate_text("The Industrial Revolution began in", original_model))

print("\nAfter fine-tuning:")
print(generate_text("The Industrial Revolution began in", model))